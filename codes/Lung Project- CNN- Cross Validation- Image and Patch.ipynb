{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "\n",
    "import os\n",
    "# to run on GPU, comment the following tow lines \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import pandas\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import pylab\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.models import Model\n",
    "from util5 import read_data_dual_input # image and patch\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import plot_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "\n",
    "# train\n",
    "\n",
    "csvPath = 'OLD2/Master2M.csv'\n",
    "imagePath = '/home/maalidoost/OLD2/Seg2M'\n",
    "patchPath = '/home/maalidoost/OLD2/Seg2Mask'\n",
    "\n",
    "inputSize = (224,224)\n",
    "patchSize = (64, 64)\n",
    "split_ratio = 0.2\n",
    "batch_size = 16\n",
    "learning_rate = 1e-6\n",
    "epoch_size = 2\n",
    "nodes = 1024 # number of nodes for the fc layer\n",
    "drop_out = 0.5 # after the fc layer\n",
    "k_fold = 2\n",
    "\n",
    "input_1 = (224, 224, 3)\n",
    "input_2 = (64, 64, 3)\n",
    "\n",
    "# test\n",
    "\n",
    "test_dir = '/home/maalidoost/Seg2'\n",
    "csvTest = 'Master2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded: 16 positive,15 negatve.\n",
      "Images loaded: 31; Masks loaded: 31\n",
      "\n",
      "multiple inputs model\n",
      "\n",
      "fold 1, image size: (90, 224, 224, 3)\n",
      "fold 1, patch size: (90, 64, 64, 3)\n",
      "fold 1, label size: (90,)\n",
      "fold 1, val_image size: (96, 224, 224, 3)\n",
      "fold 1, val_patch size: (96, 64, 64, 3)\n",
      "fold 1, val_label size: (96,)\n",
      "\n",
      "fold 2, image size: (96, 224, 224, 3)\n",
      "fold 2, patch size: (96, 64, 64, 3)\n",
      "fold 2, label size: (96,)\n",
      "fold 2, val_image size: (90, 224, 224, 3)\n",
      "fold 2, val_patch size: (90, 64, 64, 3)\n",
      "fold 2, val_label size: (90,)\n",
      "\n",
      "\n",
      "making the model for the first input\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "making the model for the second input\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "fold 1:\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.7471 - acc: 0.5125 - val_loss: 0.6844 - val_acc: 0.5417\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 3s 613ms/step - loss: 0.7648 - acc: 0.4811 - val_loss: 0.6743 - val_acc: 0.6250\n",
      "acc: 62.50%\n",
      "auc: 67.32%\n",
      "\n",
      "fold 2:\n",
      "Epoch 1/2\n",
      "6/6 [==============================] - 2s 380ms/step - loss: 0.7852 - acc: 0.5208 - val_loss: 0.6684 - val_acc: 0.6333\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 2s 354ms/step - loss: 0.6982 - acc: 0.5729 - val_loss: 0.6660 - val_acc: 0.6778\n",
      "acc: 67.78%\n",
      "auc: 67.71%\n",
      "\n",
      "tot_acc_avg: 65.14% (+/- 2.64%)\n",
      "tot_auc_avg: 67.51% (+/- 0.20%)\n"
     ]
    }
   ],
   "source": [
    "# preparing the data to train\n",
    "\n",
    "random.seed(7)\n",
    "\n",
    "train_images, train_patches, train_labels, val_images, val_patches, val_labels = \\\n",
    "read_data_dual_input(csvPath, imagePath, patchPath, inputSize, patchSize, split_ratio,\n",
    "                     aug_rotate = 6, kfold = k_fold, outchannels = 3)\n",
    "\n",
    "print(\"\")\n",
    "print(\"multiple inputs model\")\n",
    "print(\"\")\n",
    "\n",
    "for i in range(k_fold):\n",
    "    print ('fold %d, image size:' % (i+1), train_images[i].shape)\n",
    "    print ('fold %d, patch size:' % (i+1), train_patches[i].shape)\n",
    "    print ('fold %d, label size:' % (i+1), train_labels[i].shape)\n",
    "    print ('fold %d, val_image size:' % (i+1), val_images[i].shape)\n",
    "    print ('fold %d, val_patch size:' % (i+1), val_patches[i].shape)\n",
    "    print ('fold %d, val_label size:' % (i+1), val_labels[i].shape)\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"making the model for the first input\")\n",
    "\n",
    "# using the pretrained model for training\n",
    "\n",
    "conv_base_1 = VGG16(weights = 'imagenet', include_top = False, input_shape = input_1)\n",
    "\n",
    "for layer in conv_base_1.layers:\n",
    "    layer.name = layer.name + str(\"_1\")\n",
    "\n",
    "#conv_base_1.summary()\n",
    "\n",
    "# Creating dictionary that maps layer names to the layers\n",
    "#layer_dict = dict([(layer.name, layer) for layer in conv_base.layers])\n",
    "# Getting output tensor of the last VGG layer that we want to include\n",
    "#lay = layer_dict['block5_pool'].output\n",
    "#print (lay)\n",
    "\n",
    "for layer in conv_base_1.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "\n",
    "#for layer in conv_base_1.layers:\n",
    "#    print(layer, layer.trainable)\n",
    "\n",
    "inp_1 = layers.Input(shape = input_1)\n",
    "conv_base_out_1 = conv_base_1(inp_1)\n",
    "flat_1 = layers.Flatten()(conv_base_out_1)\n",
    "#print(flat_1)\n",
    "\n",
    "print(\"\")\n",
    "print(\"making the model for the second input\")\n",
    "print(\"\")\n",
    "# using the pretrained model for training\n",
    "\n",
    "#conv_base_2 = VGG16(weights = 'imagenet', include_top = False, input_shape = input_2)\n",
    "conv_base_2 = VGG19(weights = 'imagenet', include_top = False, input_shape = input_2)\n",
    "#conv_base_2 = ResNet50(weights = 'imagenet', include_top = False, input_shape = input_2)\n",
    "\n",
    "for layer in conv_base_2.layers:\n",
    "    layer.name = layer.name + str(\"_2\")\n",
    "\n",
    "#conv_base_2.summary()\n",
    "\n",
    "# Creating dictionary that maps layer names to the layers\n",
    "#layer_dict = dict([(layer.name, layer) for layer in conv_base.layers])\n",
    "# Getting output tensor of the last VGG layer that we want to include\n",
    "#lay = layer_dict['block5_pool'].output\n",
    "#print (lay)\n",
    "\n",
    "for layer in conv_base_2.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "\n",
    "#for layer in conv_base_2.layers:\n",
    "#    print(layer, layer.trainable)\n",
    "\n",
    "inp_2 = layers.Input(shape = input_2)\n",
    "conv_base_out_2 = conv_base_2(inp_2)\n",
    "flat_2 = layers.Flatten()(conv_base_out_2)\n",
    "#print(flat_2)\n",
    "\n",
    "# making the FC layers of the model\n",
    "\n",
    "concat = layers.merge.concatenate([flat_1, flat_2])\n",
    "dense1 = layers.Dense(nodes, activation = 'relu')(concat)\n",
    "dense1 = layers.Dropout(drop_out)(dense1)\n",
    "output = layers.Dense(1, activation= 'sigmoid')(dense1)\n",
    "\n",
    "# creating the model with two inputs\n",
    "\n",
    "model = models.Model(inputs = [inp_1, inp_2], outputs = output)\n",
    "#print(model.summary())\n",
    "\n",
    "# plotting the model graph\n",
    "\n",
    "plot_model(model, to_file='multiple_inputs.png')\n",
    "\n",
    "# compiling the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.adam(lr=learning_rate), metrics=['acc'])\n",
    "\n",
    "# defining k-fold cross validation test harness\n",
    "\n",
    "cvscores1 = []\n",
    "cvscores2 = []\n",
    "i = 1\n",
    "for j in range(k_fold):\n",
    "    X1 = train_images[j]\n",
    "    X2 = train_patches[j]\n",
    "    y = train_labels[j]\n",
    "    X1_t = val_images[j]\n",
    "    X2_t = val_patches[j]\n",
    "    y_t = val_labels[j]\n",
    "\n",
    "    # data augmentation\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "\n",
    "    #val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # training the model\n",
    "\n",
    "    print('fold %d:' % (i))\n",
    "    H = model.fit_generator(\n",
    "        train_datagen.flow([X1, X2], y, batch_size = batch_size),\n",
    "        steps_per_epoch= len(X1) // batch_size,\n",
    "        epochs = epoch_size,\n",
    "        validation_data = ([X1_t, X2_t], y_t),\n",
    "        validation_steps = len(X1_t) // batch_size)\n",
    "\n",
    "    # computing the accuracy metric for this CV fold\n",
    "    \n",
    "    scores = model.evaluate([X1_t, X2_t], y_t, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores1.append(scores[1] * 100)\n",
    "    \n",
    "    # computing the AUC metric for this CV fold\n",
    "\n",
    "    preds = model.predict([X1_t, X2_t])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_t, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(\"%s: %.2f%%\" % ('auc', roc_auc*100));\n",
    "    cvscores2.append(roc_auc*100)\n",
    "    i = i + 1\n",
    "    print('');\n",
    "\n",
    "print(\"tot_acc_avg: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores1), np.std(cvscores1)));\n",
    "print(\"tot_auc_avg: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores2), np.std(cvscores2)));\n",
    "\n",
    "# saving the model\n",
    "\n",
    "model.save_weights('model_weights_lung_pro5_cv_multiple.h5')\n",
    "model.save('model_lung_pro5_cv_multiple.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'model_lung_pro_2.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-665a92d9a294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# loading the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_lung_pro_2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/build/h5py-nQFNYZ/h5py-2.6.0/h5py/_objects.c:2577)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/build/h5py-nQFNYZ/h5py-2.6.0/h5py/_objects.c:2536)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/build/h5py-nQFNYZ/h5py-2.6.0/h5py/h5f.c:1811)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'model_lung_pro_2.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "\n",
    "# loading the model\n",
    "\n",
    "model = load_model('model_lung_pro_2.h5')\n",
    "#model.summary()\n",
    "\n",
    "# getting X and y for testing\n",
    "\n",
    "file_list2 = os.listdir(test_dir)\n",
    "test_imgs = [test_dir + \"/\" + \"{}\".format(i) for i in file_list2]\n",
    "#print(\"No. of test images = \", len(test_imgs))\n",
    "\n",
    "X_test = []\n",
    "IMG_SIZE = 224\n",
    "for image in test_imgs:\n",
    "    X_test.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_CUBIC))\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test/255.0\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "\n",
    "df = pandas.read_csv(csvTest)\n",
    "#print('shape of the dataframe:', df.shape)\n",
    "#print(df.head(2))\n",
    "\n",
    "na = df.loc[:,'File']\n",
    "la = df.loc[:,'Progression']\n",
    "na = np.array(na)\n",
    "la = np.array(la)\n",
    "I = np.argsort(na)\n",
    "na = na[I]\n",
    "la = la[I]\n",
    "y_test = la\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,4)})\n",
    "sns.countplot (y_test)\n",
    "plt.title(\"Labels\")\n",
    "print (\"shape of y_test:\", y_test.shape)\n",
    "\n",
    "# model prediction\n",
    "\n",
    "preds_test = model.predict(X_test, verbose=1)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "predictions_test = preds_test_t [:, 0]\n",
    "print(\"\")\n",
    "\n",
    "print(\"Predicted labels:\", predictions_test)\n",
    "print(\"\")\n",
    "print(\"True labels:\", y_test)\n",
    "print(\"\")\n",
    "\n",
    "com = np.isclose(predictions_test, y_test.T)\n",
    "print (com)\n",
    "\n",
    "#true_prediction_number = 1 * (com == 'True')\n",
    "#print(true_prediction_number)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_test)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
